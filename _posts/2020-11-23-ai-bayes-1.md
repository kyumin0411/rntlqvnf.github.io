---
title: "&#91;AI&#93; Bayes' Nets (1)"
categories:
  - Lecture Notes
tags:
  - ai
  - csed442
use_math: true
toc: true
---

> 이 글은 포스텍 이근배 교수님의 CSED442 인공지능 수업의 강의 내용과 자료를 기반으로 하고 있습니다.

오랜만에 쓰는 인공지능 글이다.

하도 일이 많아서 글이 많이 밀렸다.

이번주는 과제가 없어서 이번주 안에 밀린 양을 다 쓸 예정이다.

# 1. Probability

베이즈 네트워크에 대해 들어가기 전에, 베이즈 네트워크의 핵심이 되는 **독립**에 대해서 배워야 한다.

## Independence

두 확률 변수가 독립이면 아래와 같은 관계가 성립한다.

- $\forall x,y : P(x,y)=P(x)P(y)$

- $\forall x,y : P(x\vert y)=P(x)$

이를 아래와 같은 기호로 표시한다.

$X \mathrel{\unicode{x2AEB}} Y$

## Conditional Independence

$X$가 $Z$가 주어졌을 때 $Y$에 대해 독립이라면 아래와 같은 관계가 성립한다.

- $\forall x,y,z: P(x,y\vert z)=P(x\vert z)P(y\vert z)$

- $\forall x,y,z: P(x\vert z,y)=P(x\vert z)$

이를 **조건부 독립**이라고 하며, 아래와 같은 기호로 표시한다.

$X\mathrel{\unicode{x2AEB}} Y \vert Z$

개인적으로 이 개념을 처음 봤을 때 직관적으로 와닿지 않았었다.

우선 수식의 의미부터 간단히 알아보자.

> $\forall x,y,z: P(x,y\vert z)=P(x\vert z)P(y\vert z)$

이 수식은 독립의 $\forall x,y : P(x,y)=P(x)P(y)$ 수식과 거의 유사한 의미이다.

다만, $z$가 주어진 경우에만 쪼갤 수 있다! 라고 이해하면 된다.

> $\forall x,y,z: P(x\vert z,y)=P(x\vert z)$

이 수식도 독립의 $\forall x,y : P(x\vert y)=P(x)$ 수식과 거의 유사한 의미이다.

다만, $z$가 주어진 경우에만 $y$를 무시할 수 있다! 라고 이해하면 된다.

수식적으로 이해는 했으니, 직관적으로 조건부 독립이 무슨 소리인지 이해해보자.

![BN example][I_1]

Traffic, Umbrella, Raining 이라는 세 개의 사건이 주어졌다고 해보자.

세 사건 간의 연관관계는 위의 그래프와 같다.

Raining이 주어지지 않았을 때, Traffic과 Umbrella는 독립일까?

**그렇지 않다**. 

예를 들어, Traffic이 있으면 해보자. 그러면 Raining이 있을 확률이 올라가므로, Umbrella의 확률도 올라가게 된다.

반대로 Traffic이 없으면 raining의 확률이 내려가고, 이에 따라 umbrella의 확률도 내려간다.

그렇다면 raining이 주어지면, Traffic과 Umbrella는 독립일까?

**그렇다.**

예를 들어, Raining이 있다고 해보자. 그러면 Traffic이 어떻던 간에, 이미 raining은 정해져 있으므로 traffic의 사건이 umbrella에게 영향을 끼치 못한다.

따라서 우리는 Umbrella는 Raining이 주어졌을 때, T에 대해 독립이다 라고 말할 수 있는 것이다.

PPT에 있는 chain rule과 ghostbuster 부분은 이후 베이즈 네트워크를 이해하면 자연스레 이해가 될 것이므로 생략한다.

# 2. Bayes' Nets

## Probabilistic Model

우리가 만들고자 하는 것은 확률 모델이다.

모델이란 현실을 추상화한 것을 말하는데, 모델링을 하는 이유는 좀 더 쉽게 계산을 하기 위해서이다.

예를 들어 자동차의 움직임에 대한 모델링을 한다고 하자.

모델을 만드는 가장 직관적인 방법은 joint distribution table을 만드는 것이다.

그러면 자동차의 움직임에 관여하는 요소들; 배터리 수명, 배터리 충전 상태, 기름 충전 상태, 경고등 상태 등등 을 추려낸 다음, 이것에 대해서 full joint distribution table을 만들면 된다.

|배터리 수명 | 기름 충전 상태 | 경고등 | ... | P(B, O, L, ...)|
|:---:|:---:|:---:|:---:|:---:|
|0|0|0|...|0.01|
|1|1|1|...|0.01|
|2|2|1|...|0.01|

이런 식으로 말이다.

그런데 이게 현실적일까? 요소가 한 10개만 되어도 한눈에 담을 수 없을 정도로 표가 커질 것이다.

그래서 모델을 표시할 다른 방법이 필요했고, 그게 바로 베이즈 네트워크이다.

베이즈 네트워크의 핵심은 독립을 이용해서 좀 더 간결하게 확률 모델을 표현하는 것이다.

아직 베이즈 네트워크를 배우진 않았지만, 한번 자동차의 움직임에 대한 모델을 베이즈 네트워크를 이용해서 표시해보면 대충 아래처럼 나온다.

![Car][I_2]

full joint distribution 방식은 사건 간의 연관관계를 전혀 활용하지 않았다.

베이즈 네트워크는 사건 간의 연관관계(local interaction)를 적극적으로 활용한다.

예를 들어, 위의 베이즈 네트워크는 아래의 연관관계를 활용하고 있다.

- 베터리가 죽는 사건은 배터리의 수명과 관계가 있다.

- 배터리가 충전이 안되어있는 사태는 배터리가 죽었거나 충전이 안되는 사건과 관계가 있다.

이때 이 관계를 독립을 사용해서 formal하게 표현할 수 있기 때문에 우리가 앞서 독립에 대해 배웠던 것이다.

## Intuition

![Initution][I_3]

우리가 앞선 예제에서 베이즈 네트워크에 대해 배웠던 것을 정리하면 아래와 같다.

- 노드는 확률 변수(interaction)를 의미한다.
- 화살표는 연관관계(interaction)를 의미한다.

나중에 배우겠지만, 정확하게는 화살표는 conditional independence를 의미한다.

잠깐 **퀴즈**! 베이즈 네트워크가 full joint distribution과 다를바가 없을 땐 언제일까?

정답은 **전부 독립일 때**이다.

![Abs][I_4]

이렇게 모든 확률 변수가 독립이라면, 확률 변수 간에 어떠한 관계도 없으므로 베이즈 네트워크가 함축할 수 있는 정보가 없다.

**퀴즈 2**! 아래 상황의 베이즈 네트워크를 구하자.

![Quiz][I_5]

잘 생각해보면 꽤 당연하다. 답은 이렇다.

![Ans][I_6]

이쯤되면 베이즈 네트워크가 확률변수 간의 연관관계를 표현한다는 것을 알았을 것이다.

그럼 이제 베이즈 네트워크의 formal한 정의에 대해서 알아보자.

## Semantics

베이즈 네트워크는 세 가지 성질에 의해 정의된다.

- A set of nodes, one per variable $X$
  
- A directed acyclidc graph
  
- A conditional distibution for each node
  
  이전에는 연관관계라고 모호하게 표현했던 것을 좀 더 정확하게 정의할 수 있다.

  ![Sem][I_7]

  어떤 노드 $X$가 $A_1, \cdots, A_n$를 부모로 가진다는 말은 $X$가 $P(P\vert A_1,  \cdots , A_n)$라는 conditional distribution을 가진다는 의미이다.

곧, 베이즈 네트워크란 **그래프 + Local conditional probabilities**라고 할 수 있다.

## Probabilities in BNs

한 가지 놀라운 사실은 베이즈 네트워크는 joint distribution을 implicit하게 인코딩하고 있다는 점이다.

분명 우리가 정의한건 Local conditional distribution 밖에 없는데, 이것들을 잘 조합하면 joint distribution을 만들어낼 수 있다는 것이다.

만드는 방법은 간단하다. Local conditional distribution을 전부 다 곱하면 된다!

![Joint][I_8]

근데 그렇다고 막 아무 순서대로 곱해서 되는 건 아니다. **되도록 하는 순서**가 있다.

간단한 예제를 통해 이해도를 높여보자.

![Example][I_9]

위와 같은 베이즈 네트워크가 주어졌을 때, joint distribution을 어떻게 구할 수 있을까?

아무 순서대로나 막 곱하면 당연히 안되고, $x_1=cavity$이어야만 한다.

왤까? 수식의 증명을 통해 알아보자.

증명 과정은 간단하다.

1. Apply chain rule

   ![Chain][I_10]

2. Assume conditional independences
   
   ![Assume][I_11]

   **Assume** 한다는 표현에 주의를 기울여야 한다.

   ![Sem][I_7]

   우리가 베이즈 네트워크에서 정의한 것은 각 노드가 parent에 대한 conditional distribution을 가진다는 것 밖에 없다.

   위 수식처럼 노드에 순서를 부여하지도 않았고, $x_i$의 부모가 $x_1, x_2, \cdots, x_{i-1}$ 중에 있다는 정의는 하지 않았다.

   그렇기에 **Assume** 한 것이다.

   Formal하게는 아래의 수식을 assume 하는 것이라 할 수 있다.

   $x_i \mathrel{\unicode{x2AEB}} \lbrace x_1, x_2, \cdots x_{i-1} \rbrace - parents(X_i) \vert parent(X_i)$

   곧, joint distribution을 구하기 위해서는 부모에 낮은 숫자를 부여하고, 위에서부터 아래로 내려오면서 local distribution을 구해서 곱해야 한다.

3. Consequence
   
   ![Joint][I_8]

이로부터 알 수 있는 사실이 있다.

> Not every BN can represent every joint distribution

베이즈 네트워크는 자신이 인코딩하고 있는 joint distribution과 일치하지 않는 joint distribution을 표현할 수 없다는 얘기이다.

![Encode][I_12]

예를 들어 위 그래프는 해당 conditional indepdency를 만족하는 joint distribution만 표현할 수 있다.

자 그럼 간단한 예제를 풀어보자.

![Example_2][I_13]

이 친구의 joint distribution $P(+b,-e,+a,-j,+m)$은 어떻게 구할 수 있을까?

답은 $P(+b)P(-e)P(+a\vert +b, -e)P(-j\vert +a)P(+m \vert+a)$이다.

왜 이렇게 구해야 하는지 이해가 가면 이 파트를 잘 이해한 것이다.

## Causality

한 가지 생각해봐야 할 점은 화살표가 의미하는게 인과관계(casuality)일까 아님 상관관계(correlation)일까?

흔히 인과관계라고 생각하기 쉬운데, 꼭 인과관계를 표현하고 있는 것은 아니다.

//여기부터 더 적기

# 3. Conditional Independences in BNs

## D-separation

### Casual Chain

### Common caues

### Common effect

## The General Case

### Reachability

### Active / Inactive Paths

#### Examples

## 

# 4. Summary

[I_1]: /assets/lecture/ai/bn_1/ci_example.PNG
[I_2]: /assets/lecture/ai/bn_1/car.PNG
[I_3]: /assets/lecture/ai/bn_1/int.PNG
[I_4]: /assets/lecture/ai/bn_1/abs.PNG
[I_5]: /assets/lecture/ai/bn_1/quiz.PNG
[I_6]: /assets/lecture/ai/bn_1/ans.PNG
[I_7]: /assets/lecture/ai/bn_1/sem.PNG
[I_8]: /assets/lecture/ai/bn_1/joint.PNG
[I_9]: /assets/lecture/ai/bn_1/ex.PNG
[I_10]: /assets/lecture/ai/bn_1/chain.PNG
[I_11]: /assets/lecture/ai/bn_1/assume.PNG
[I_12]: /assets/lecture/ai/bn_1/encode.PNG
[I_13]: /assets/lecture/ai/bn_1/example_2.PNG